{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required modules\n",
    "\n",
    "#NLP Modules\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "#Statistics Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the text file\n",
    "\n",
    "fp = open('History-Class6.txt', 'r')\n",
    "text = fp.read()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Buddhist', 'Vajjis', 'Atranjikhera', 'Forests', 'today', 'Bihar', 'Priests', 'Ganga', 'Pataliputra', 'RepublicThis', 'Vassakara', 'Vedas', 'Mahajanapadas', 'Europe', 'Appointments', 'Mahavira', 'Citizens', 'Egypt', 'Indian', 'Vedic', 'Bimbisara', 'Chaityas', 'Ajatasattu', 'Delhi', 'Gupta', 'Vajji', 'Meerut', 'Macedonia', 'Athens', 'India', 'Etah', 'Yamuna', 'Rajgir', 'Alexander', 'Kings', 'Rajagriha', 'Rajas', 'Shankaran', 'Hastinapur', 'Buddha'}\n"
     ]
    }
   ],
   "source": [
    "#Add named entities to the raw keywords list.\n",
    "\n",
    "doc = nlp(text)\n",
    "ent_list = set([ent.text for ent in doc.ents if not ent.label_ in [\"ORDINAL\", \"CARDINAL\"] \n",
    "                and nlp.vocab[ent.text].is_alpha \n",
    "                and len((ent.text).split())<2])\n",
    "\n",
    "#Create a dictionary to hold keywords to demarcate.\n",
    "#Later, will have the key-value pairs section segmented with the key being the main clue.\n",
    "raw_keywords = {'Entities': ent_list}  \n",
    "\n",
    "print(ent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE-PROCESSED TEXT:\n",
      "\n",
      "['kingdoms', 'kings', 'and', 'an', 'early', 'republic', 'election', 'day', 'shankaran', 'woke', 'up', 'to', 'see', 'his', 'grandparents', 'all', 'ready', 'to', 'go', 'and']\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing the text.\n",
    "\n",
    "# 1. Removing punctuations\n",
    "text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "# 2. Convert to lower case\n",
    "text = text.lower()\n",
    "\n",
    "# 3. Remove tags\n",
    "text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "# 4. Remove special characters\n",
    "text = re.sub(\"(\\W)+\", \" \", text)\n",
    "\n",
    "# 5. Convert to list\n",
    "text = text.split()\n",
    "\n",
    "print('PRE-PROCESSED TEXT:', text[:20], sep = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP WORDS ADDED:\n",
      "\n",
      "['chapter', 'ncert', 'class', 'vi', 'people']\n"
     ]
    }
   ],
   "source": [
    "#Extract common words / word-count\n",
    "\n",
    "word_c = {}                                              #Create a dictionary to hold the frequencies of every word.\n",
    "for word in text:\n",
    "    if word in word_c.keys():\n",
    "        word_c[word] += 1\n",
    "    else:\n",
    "        word_c[word] = 1\n",
    "word_c\n",
    "\n",
    "#Choose mechanism to set threshold to decide stop words.\n",
    "# a. Divide the total number of words in the text by x\n",
    "# b. Divide the number of words in the dictionary by x\n",
    "#Chosen mechanism = percentile (97th)\n",
    "\n",
    "frequencies = [v for v in word_c.values()]\n",
    "threshold = np.percentile(frequencies, 97)\n",
    "\n",
    "#Creating custom stop words list.\n",
    "\n",
    "s_w = ['chapter', 'ncert', 'class', 'vi']                #NEED TO POSSIBLY INCLUDE THE CHAPTER NAME IN THIS LIST\n",
    "for k, v in word_c.items():\n",
    "    if v>threshold:\n",
    "        s_w.append(k)\n",
    "\n",
    "#Adding custom stop words\n",
    "\n",
    "new_sw = []\n",
    "def custom_stop_words(word):\n",
    "    if not nlp.vocab[word].is_stop:\n",
    "        new_sw.append(word)                               #Just to keep a record of the custom words added.\n",
    "        nlp.Defaults.stop_words.add(word)\n",
    "        nlp.vocab[word].is_stop = True\n",
    "\n",
    "for word in s_w:\n",
    "    custom_stop_words(word)\n",
    "\n",
    "print(\"STOP WORDS ADDED:\", new_sw, sep = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEMMATIZED LIST:\n",
      "\n",
      "['kingdom', 'king', 'and', 'an', 'early', 'republic', 'election', 'day', 'shankaran', 'wake', 'up', 'to', 'see', '-PRON-', 'grandparent', 'all', 'ready', 'to', 'go', 'and']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "\n",
    "doc = nlp(\" \".join(text))\n",
    "lemmatized_list = []\n",
    "for token in doc:\n",
    "    lemmatized_list.append(token.lemma_)\n",
    "    \n",
    "print(\"LEMMATIZED LIST:\", lemmatized_list[:20], sep = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILTERED TEXT:\n",
      "\n",
      "['kingdom', 'king', 'early', 'republic', 'election', 'day', 'shankaran', 'wake', 'grandparent', 'ready', 'vote', 'want', 'reach', 'polling', 'booth', 'shankaran', 'want', 'know', 'excited', 'somewhat']\n"
     ]
    }
   ],
   "source": [
    "#Removing stop words\n",
    "\n",
    "doc = nlp(\" \".join(lemmatized_list))\n",
    "filtered_list = []\n",
    "for token in doc:\n",
    "    if not token.is_stop:\n",
    "        if not token.is_punct:\n",
    "            if not token.text == '-PRON-':\n",
    "                filtered_list.append(token.text)\n",
    "\n",
    "print(\"FILTERED TEXT:\",filtered_list[:20], sep = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word  Freq\n",
      "0        king    14\n",
      "1        raja    14\n",
      "2       ruler    13\n",
      "3     perform    13\n",
      "4        year    12\n",
      "5      priest    11\n",
      "6   sacrifice    10\n",
      "7     kingdom     9\n",
      "8      ritual     9\n",
      "9     magadha     9\n",
      "10       want     8\n",
      "11        man     8\n",
      "12   janapada     8\n",
      "13       know     7\n",
      "14       find     7\n",
      "15       mean     7\n",
      "16      varna     7\n",
      "17  different     7\n",
      "18      study     7\n",
      "19  important     7\n"
     ]
    }
   ],
   "source": [
    "#Text Feature Extraction\n",
    "\n",
    "cv=CountVectorizer(max_df=0.8,stop_words=nlp.Defaults.stop_words, max_features=10000, ngram_range=(1,3))\n",
    "X=cv.fit_transform(filtered_list)\n",
    "\n",
    "#Most frequently occuring words\n",
    "def get_top_n_words(text, n=None):\n",
    "    vec = CountVectorizer().fit(text)\n",
    "    bag_of_words = vec.transform(text)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "top_words = get_top_n_words(filtered_list, n=20)\n",
    "\n",
    "\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "print(top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Entities': {'Buddhist', 'Vajjis', 'Atranjikhera', 'Forests', 'today', 'Bihar', 'Priests', 'Ganga', 'Pataliputra', 'RepublicThis', 'Vassakara', 'Vedas', 'Mahajanapadas', 'Europe', 'Appointments', 'Mahavira', 'Citizens', 'Egypt', 'Indian', 'Vedic', 'Bimbisara', 'Chaityas', 'Ajatasattu', 'Delhi', 'Gupta', 'Vajji', 'Meerut', 'Macedonia', 'Athens', 'India', 'Etah', 'Yamuna', 'Rajgir', 'Alexander', 'Kings', 'Rajagriha', 'Rajas', 'Shankaran', 'Hastinapur', 'Buddha'}, 'Extras': ['raja', 'king', 'ruler', 'perform', 'year', 'priest', 'sacrifice', 'ritual', 'magadha', 'kingdom']}\n",
      "\n",
      "\n",
      "raja            0.188\n",
      "king            0.188\n",
      "ruler           0.177\n",
      "perform         0.177\n",
      "year            0.165\n",
      "priest          0.154\n",
      "sacrifice       0.142\n",
      "ritual           0.13\n",
      "magadha          0.13\n",
      "kingdom          0.13\n"
     ]
    }
   ],
   "source": [
    "#Keyword extraction tf-idf method.\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# fetch document for which keywords needs to be extracted\n",
    "doc=\" \".join(filtered_list)\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    " \n",
    "# now add the results to keyword dictionary\n",
    "\n",
    "raw_keywords['Extras'] = [k for k in keywords if not k in raw_keywords['Entities']]\n",
    "print(raw_keywords)\n",
    "\n",
    "# tf-idf values of the keywords:\n",
    "print('\\n')\n",
    "for k in keywords:\n",
    "    print(f'{k:{15}} {keywords[k]:{5}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
